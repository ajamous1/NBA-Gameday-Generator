#import libraries
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException
from bs4 import BeautifulSoup
import time
import requests
import json
import subprocess
import os
import pytz
from datetime import datetime, timedelta

#generate dropbox access token from https://dropbox.github.io/dropbox-api-v2-explorer/#files_get_temporary_upload_link
dropbox_access_token = ("")
adobe_client_secret = os.getenv("ADOBE_CLIENT_SECRET")
adobe_access_token = " "
adobe_api_key = os.getenv("ADOBE_API_KEY")




home_team_primary = [0, 0, 0]
home_team_secondary = [0, 0, 0]
away_team_primary = [0, 0, 0]
away_team_secondary = [0, 0, 0]


#Extract all data from NBALockerVision and Statmuse.com
class WebCrawler:
    def __init__(self, first_link, num):
        self.ID = num
        self.driver = self.setup_webdriver()
        self.home_team = None
        self.away_team = None
        self.record_home = None
        self.seeding_home = None
        self.record_away = None
        self.seeding_away = None
        self.last_5_games_results_home = None
        self.last_5_games_results_away = None
        self.raw_date = None
        self.time_data = None
        self.formatted_date = None
        self.first_link = first_link
        self.crawl()

    def setup_webdriver(self):
        try:
            options = Options()
            options.add_argument('--headless')
            #chrome webdriver file path
            driver_path = r'C:\Users\ahmad\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe'
            return webdriver.Chrome(executable_path=driver_path, options=options)
        except WebDriverException as e:
            print(f"Error setting up Chromedriver: {e}")
            exit(1)
 
    def crawl(self):
        global home_team_primary, home_team_secondary, away_team_primary, away_team_secondary
        

        self.driver.get(self.first_link)
        time.sleep(2)
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        #search through the html of the website to extract all relevant information
        team_names = soup.select_one('.MuiTypography-h5').text
        raw_date = soup.select_one('.MuiTypography-caption.MatchSeason').text
        location_elements = soup.select('h6.MuiTypography-h6')
        location_data = location_elements[1] if location_elements else None
        location = location_data.text.strip() if location_data else "N/A"
        self.time_data = soup.select_one('.MuiTypography-caption.MatchTime').text
        time_data = soup.select_one('.MuiTypography-caption.MatchTime').text
        formatted_time = self.format_time(self.time_data)
        self.formatted_time = formatted_time
        jerseys_info = soup.select('.customEditionChip .MuiChip-label')
        jerseys = ', '.join([jersey.text for jersey in jerseys_info[:2]])
        opponent_info = soup.select('h5.MuiTypography-h5')
        self.away_team = opponent_info[1].text.strip() if opponent_info else "N/A"
        self.home_team = team_names.replace(self.away_team, "").strip()
        formatted_date = self.format_date(raw_date)
        self.formatted_date = formatted_date
        global home_jersey, away_jersey
        home_jersey, away_jersey = jerseys.split(', ')
        self.record_home, self.seeding_home = self.get_record_and_seeding(self.home_team)
        self.record_away, self.seeding_away = self.get_record_and_seeding(self.away_team)
        self.last_5_games_results_home = self.get_last_5_games_results(self.home_team)
        self.last_5_games_results_away = self.get_last_5_games_results(self.away_team)


        #extract team colors from nba_colors json file, generated by colorextraction.py
        with open(r"C:\Users\ahmad\NBA-Gameday-Generator\nba_colors.json", 'r') as f:
            nba_colors = json.load(f)

      
        home_team_primary = nba_colors[self.home_team.title()][home_jersey]["first_color"]
        home_team_secondary = nba_colors[self.home_team.title()][home_jersey]["second_color"]
        away_team_primary = nba_colors[self.away_team.title()][away_jersey]["first_color"]
        away_team_secondary = nba_colors[self.away_team.title()][away_jersey]["second_color"]
  


        
       

     
        

        self.process_and_output_data(self.home_team, self.away_team, jerseys, location, formatted_date, time_data)

    
    #output data for console (optional)
    def process_and_output_data(self, home_team, away_team, jerseys, location, date, time_data):
        try:
            print(f"Processed data:\nHome Team: {home_team}\nAway Team: {away_team} \nJerseys: {jerseys}\nLocation: {location}\nDate: {date}\nTime: {time_data}")
            print(f"Home Record: {self.record_home}\nHome Seeding: {self.seeding_home}")
            print(f"Home Last 5 Games: {self.last_5_games_results_home}")
            print(f"Away Record: {self.record_away}\nAway Seeding: {self.seeding_away}")
            print(f"Away Last 5 Games: {self.last_5_games_results_away}")
        except UnicodeEncodeError:
            print("UnicodeEncodeError: Unable to print some characters")

    def format_date(self, raw_date):
        formatted_date = raw_date.split(', ')[1]
        return formatted_date

    #format time and convert from ET to CT (Minnesota operates on Central Time)
    def format_time(self, raw_time):
       
        raw_time = " ".join(raw_time.split())
       
        time, am_pm, timezone = raw_time.split()
        
        time_object = datetime.strptime(time + ' ' + am_pm, '%I:%M %p')
      
        if timezone == 'EST':
            time_object = time_object - timedelta(hours=1) 
    
        formatted_time = time_object.strftime('%I:%M %p\nCT').lstrip('0')
        return formatted_time

    def quit_webdriver(self):
        self.driver.quit()

    #fetch the last 5 games from statmuse.com
    def get_last_5_games_results(self, team_name):
        base_url = f"https://www.statmuse.com/nba/ask/{team_name.lower().replace(' ', '-')}-last-5-games"
        response = requests.get(base_url)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            
            last_5_games = []

            game_result_divs = soup.find_all('div', {'class': 'w-5'})
            for div in game_result_divs:
                result = div.text.strip()
                last_5_games.append(result)

            return ''.join(last_5_games)
        else:
            print(f"Failed to fetch data from {base_url}")
            return None

    #fetch the record and standings for each team from statmuse.com
    def get_record_and_seeding(self, team_name):
        base_url = f"https://www.statmuse.com/ask/{team_name.lower().replace(' ', '-')}"
        response = requests.get(base_url)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            
            record_element = soup.select_one('.whitespace-nowrap span:nth-of-type(1)')
            seeding_element = soup.select_one('.whitespace-nowrap span:nth-of-type(3)')
            record = record_element.text.strip() if record_element else "N/A"
            seeding = seeding_element.text.strip() if seeding_element else "N/A"

            return record, seeding
        else:
            print(f"Failed to fetch data from {base_url}")
            return None, None

#allow user to input team name
if __name__ == "__main__":
    team_name = input("Enter the team name: ")
    team_crawler = WebCrawler(fr"https://lockervision.nba.com/team/{team_name.lower().replace(' ', '-')}", 1)

#adobe curl request
    adobe_curl_command = [
        'curl',
        '-X', 'POST',
        'https://ims-na1.adobelogin.com/ims/token/v3',
        '-H', 'Content-Type: application/x-www-form-urlencoded',
        '-d', f'grant_type=client_credentials&client_id={adobe_api_key}&client_secret={adobe_client_secret}&scope=AdobeID,openid'
    ]
    try:
        adobe_output = subprocess.check_output(adobe_curl_command)
        adobe_output_json = json.loads(adobe_output)
        adobe_access_token = adobe_output_json.get('access_token')
    except subprocess.CalledProcessError as e:
        print(f"Error generating Adobe access token: {e}")
        exit(1)

   #download Base.psd file through the dropbox API
    download_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', '{"path":"/Gameday Generator/Base1.psd"}'
    ]
    try:
        download_output = subprocess.check_output(download_curl_command)
        download_output_json = json.loads(download_output)
        download_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)

    #download font through the dropbox API
    download_font_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', '{"path":"/Lovelo/Lovelo-Black.ttf"}'
    ]
    try:
        download_output = subprocess.check_output(download_font_curl_command)
        download_output_json = json.loads(download_output)
        download_font_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)

    #generate upload link to prepare for upcoming changes to include in the updated JSON file
    upload_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_upload_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', '{"commit_info":{"path":"/Gameday Generator/Base1.psd","mode":{".tag":"overwrite"}}}'
    ]
    try:
        upload_output = subprocess.check_output(upload_curl_command)
        upload_output_json = json.loads(upload_output)
        upload_link = upload_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating upload link: {e}")
        exit(1)

    #break down the last 5 results again to modify each individual layer. This helps with singling out each layer
    team_crawler.last_5_games_results_home = list(team_crawler.last_5_games_results_home)
    team_crawler.last_5_games_results_away = list(team_crawler.last_5_games_results_away)
    try:
        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\actions_request.json', 'r') as file:
            data = json.load(file)
             
   
      
        layer_name = None 
        for i in range(0, len(data["options"]["actionJSON"]), 2):
            select_action = data["options"]["actionJSON"][i]
            set_action = data["options"]["actionJSON"][i+1]
                    
            
            layer_name = select_action["_target"][0]["_name"]
    

    #modify the JSON file accordingly to change the colors of the teams
            if layer_name == "W/L (1) Home" and team_crawler.last_5_games_results_home[0] != "L":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "W/L (2) Home" and team_crawler.last_5_games_results_home[1] != "L":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "W/L (3) Home" and team_crawler.last_5_games_results_home[2] != "L":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "W/L (4) Home" and team_crawler.last_5_games_results_home[3] != "L":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "W/L (5) Home" and team_crawler.last_5_games_results_home[4] != "L":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "W/L (1) Away" and team_crawler.last_5_games_results_away[0] != "L":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
            if layer_name == "W/L (2) Away" and team_crawler.last_5_games_results_away[1] != "L":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
            if layer_name == "W/L (3) Away" and team_crawler.last_5_games_results_away[2] != "L":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
            if layer_name == "W/L (4) Away" and team_crawler.last_5_games_results_away[3] != "L":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
            if layer_name == "W/L (5) Away" and team_crawler.last_5_games_results_away[4] != "L":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
                
                            
            if layer_name == "Home Team Primary":
                        set_action["to"]["color"]["red"] = home_team_primary[0]
                        set_action["to"]["color"]["grain"] = home_team_primary[1]
                        set_action["to"]["color"]["blue"] = home_team_primary[2]
            if layer_name == "Home Team Secondary":
                        set_action["to"]["color"]["red"] = home_team_secondary[0]
                        set_action["to"]["color"]["grain"] = home_team_secondary[1]
                        set_action["to"]["color"]["blue"] = home_team_secondary[2]
            if layer_name == "Away Team Primary":
                        set_action["to"]["color"]["red"] = away_team_primary[0]
                        set_action["to"]["color"]["grain"] = away_team_primary[1]
                        set_action["to"]["color"]["blue"] = away_team_primary[2]
            if layer_name == "Away Team Secondary":
                        set_action["to"]["color"]["red"] = away_team_secondary[0]
                        set_action["to"]["color"]["grain"] = away_team_secondary[1]
                        set_action["to"]["color"]["blue"] = away_team_secondary[2]





        for i in range(len(data["options"]["actionJSON"])):
            if data["options"]["actionJSON"][i]["_obj"] == "select":
                layer_name = data["options"]["actionJSON"][i]["_target"][0]["_name"]
            elif data["options"]["actionJSON"][i]["_obj"] == "set":
                if "to" in data["options"]["actionJSON"][i] and "gradient" in data["options"]["actionJSON"][i]["to"]:
                    if layer_name == "Home Team Gradient":
                        for color_stop in data["options"]["actionJSON"][i]["to"]["gradient"]["colors"]:
                            color_stop["color"]["red"] = home_team_secondary[0]
                            color_stop["color"]["grain"] = home_team_secondary[1]
                            color_stop["color"]["blue"] = home_team_secondary[2]
                    if layer_name == "Away Team Gradient":
                        for color_stop in data["options"]["actionJSON"][i]["to"]["gradient"]["colors"]:
                            color_stop["color"]["red"] = away_team_secondary[0]
                            color_stop["color"]["grain"] = away_team_secondary[1]
                            color_stop["color"]["blue"] = away_team_secondary[2]

        #get download link
        for input_layer in data["inputs"]:
            if input_layer["href"] == "download_link": 
                input_layer["href"] = download_link  
        #get upload link
        for output_layer in data["outputs"]:
            if output_layer["href"] == "upload_link":
                output_layer["href"] = upload_link 
        #modify gradients accordingly
        for i in range(0, len(data["options"]["actionJSON"]), 2):
            select_action = data["options"]["actionJSON"][i]
            set_action = data["options"]["actionJSON"][i + 1]


        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\NEW_actions_request.json', 'w') as file:
            json.dump(data, file, indent=4)
    except Exception as e:
        print(f"Error modifying data: {e}")
        exit(1)

    #curl command for uploading color changes
    action_curl_command = [
        'curl',
        '-X', 'POST',
        'https://image.adobe.io/pie/psdService/actionJSON',
        '--header', f'Authorization: Bearer {adobe_access_token}',
        '--header', f'x-api-key: {adobe_api_key}',
        '--header', 'Content-Type: application/json',
        '--data', json.dumps(data) 
    ]
    try:
        action_response = subprocess.check_output(action_curl_command)
        print("Action API Response:", action_response.decode()) 
    except subprocess.CalledProcessError as e:
        print(f"Error executing cURL command for performing actions on JSON data: {e}")

    #delay required for dropbox to download an updated version of the Base.psd file
    time.sleep(5)
    download_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', '{"path":"/Gameday Generator/Base1.psd"}'
    ]
    
    #generate download and upload links as before
    try:
        download_output = subprocess.check_output(download_curl_command)
        download_output_json = json.loads(download_output)
        download_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)
    try:
        upload_output = subprocess.check_output(upload_curl_command)
        upload_output_json = json.loads(upload_output)
        upload_link = upload_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating upload link: {e}")
        exit(1)


 

    

   



    #modify text files in JSON file as required, with all the parsed data being applied to the poster
    try:
        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\text_request.json', 'r') as file:
            data = json.load(file)
        
        for layer in data["options"]["layers"]:
            for i in range(1, 6): 
                if layer["name"] == f"Game {i} Home":
                    layer["text"]["contents"] = team_crawler.last_5_games_results_home[i-1]
                if layer["name"] == f"Game {i} Away":
                    layer["text"]["contents"] = team_crawler.last_5_games_results_away[i-1]
            if layer["name"] == "Away Position":
                layer["text"]["contents"] = team_crawler.seeding_away
            if layer["name"] == "Home Position":
                layer["text"]["contents"] = team_crawler.seeding_home
            if layer["name"] == "Away Record":
                layer["text"]["contents"] = team_crawler.record_away
            if layer["name"] == "Home Record":
                layer["text"]["contents"] = team_crawler.record_home
            if layer["name"] == "Away Team":
                layer["text"]["contents"] = team_crawler.away_team
            if layer["name"] == "Home Team":
                layer["text"]["contents"] = team_crawler.home_team
            if layer["name"] == "Date":
                layer["text"]["contents"] = team_crawler.formatted_date
            if layer["name"] == "Time":
                layer["text"]["contents"] = team_crawler.formatted_time
        for input_layer in data["inputs"]:
            if input_layer["href"] == "download_link":  
                input_layer["href"] = download_link
        for output_layer in data["outputs"]:
            if output_layer["href"] == "upload_link":  
                output_layer["href"] = upload_link  
        for font_layer in data["options"]["fonts"]:
            if font_layer["href"] == "font_link": 
                font_layer["href"] = download_font_link  
        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\NEW_text_request.json', 'w') as file:
            json.dump(data, file, indent=4)
        
    except Exception as e:
        print(f"Error modifying data: {e}")
        exit(1)
        
  
    #generate text curl command
    text_curl_command = [
        'curl',
        '-X', 'POST',
        'https://image.adobe.io/pie/psdService/text',
        '--header', f'Authorization: Bearer {adobe_access_token}',
        '--header', f'x-api-key: {adobe_api_key}',
        '--header', 'Content-Type: application/json',
        '--data', json.dumps(data) 
    ]
    try:
        text_response = subprocess.check_output(text_curl_command)
        print("Text API Response:", text_response.decode())
    except subprocess.CalledProcessError as e:
        print(f"Error executing cURL command for editing text: {e}")
        
    #some NBA Logos vary by jersey. However, they are always consistent across Icon and Association jerseys. Therefore, they all fall under one "Default" folder in dropbox
    
    if home_jersey == "Association Edition" or home_jersey == "Icon Edition":
        home_jersey = "Default"
        
    if away_jersey == "Association Edition" or away_jersey == "Icon Edition":
        away_jersey = "Default"
      
    home_team_path = f'/team_logos/{home_jersey}/{team_crawler.home_team.lower().replace(" ", "")}.png'
    away_team_path = f'/team_logos/{away_jersey}/{team_crawler.away_team.lower().replace(" ", "")}.png'
    arena_path = f'/arenas/{team_crawler.home_team.lower().replace(" ", "")}.png'

    #another delay to download updated psd file
    time.sleep(10)
    download_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', '{"path":"/Gameday Generator/Base1.psd"}'
    ]
    try:
        download_output = subprocess.check_output(download_curl_command)
        download_output_json = json.loads(download_output)
        download_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)

    #curl commands for generating download links for home and away logos
    download_home_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', f'{{"path":"{home_team_path}"}}'
    ]

    try:
    
        download_output = subprocess.check_output(download_home_curl_command)

        download_output_json = json.loads(download_output)
        download_home_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)
 
    download_away_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', f'{{"path":"{away_team_path}"}}'
    ]

    try:
        download_output = subprocess.check_output(download_away_curl_command)
        download_output_json = json.loads(download_output)
        download_away_link = download_output_json.get('link')
    
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)

    #curl commands for generating download links for home arena
    download_arena_curl_command = [
        'curl',
        '-X', 'POST',
        'https://api.dropboxapi.com/2/files/get_temporary_link',
        '--header', f'Authorization: Bearer {dropbox_access_token}',
        '--header', 'Content-Type: application/json',
        '--data', f'{{"path":"{arena_path}"}}'
    ]
    try:
        download_output = subprocess.check_output(download_arena_curl_command)
        download_output_json = json.loads(download_output)
        download_arena_link = download_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating download link: {e}")
        exit(1)

    #generate upload link to refer to in JSON file
    
    try:
        upload_output = subprocess.check_output(upload_curl_command)
        upload_output_json = json.loads(upload_output)
        upload_link = upload_output_json.get('link')
    except subprocess.CalledProcessError as e:
        print(f"Error generating upload link: {e}")
        exit(1)

    #modify JSON file to include all relevant logos and images
    try:
        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\image_request.json', 'r') as file:
            data = json.load(file)
              
        for layer in data["options"]["layers"]:
            if layer["name"] == "Home Team Logo":
                layer["input"]["href"] = download_home_link
            if layer["name"] == "Home Logo":
                layer["input"]["href"] = download_home_link
            if layer["name"] == "Away Team Logo":
                layer["input"]["href"] = download_away_link
            if layer["name"] == "Away Logo":
                layer["input"]["href"] = download_away_link
            if layer["name"] == "Arena Logo":
                layer["input"]["href"] = download_arena_link

                
        for input_layer in data["inputs"]:
            if input_layer["href"] == "download_link": 
                input_layer["href"] = download_link  
        for output_layer in data["outputs"]:
            if output_layer["href"] == "upload_link": 
                output_layer["href"] = upload_link 
        
        with open('C:\\Users\\ahmad\\OneDrive\\Gameday Generator\\NEW_image_request.json', 'w') as file:
            json.dump(data, file, indent=4)
        
    except Exception as e:
        print(f"Error modifying data: {e}")
        exit(1)

    #generate image curl command
    image_curl_command = [
        'curl',
        '-X', 'POST',
        'https://image.adobe.io/pie/psdService/smartObject',
        '--header', f'Authorization: Bearer {adobe_access_token}',
        '--header', f'x-api-key: {adobe_api_key}',
        '--header', 'Content-Type: application/json',
        '--data', json.dumps(data)  
    ]
    try:
        text_response = subprocess.check_output(image_curl_command)
        print("Image API Response:", text_response.decode()) 
    except subprocess.CalledProcessError as e:
        print(f"Error executing cURL command for editing text: {e}")


    

        
    

               
